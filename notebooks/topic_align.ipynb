{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dec1339b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import pathlib\n",
    "import random\n",
    "from itertools import product\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "import gensim.downloader as api\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc3d50a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TopicSelector(object):\n",
    "    \"\"\"\n",
    "    Class to select topics from different topic models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        wmd_model: str = 'word2vec-google-news-300',\n",
    "        logger: Optional[logging.Logger] = None,\n",
    "        config_path: pathlib.Path = pathlib.Path(\"config/config.yaml\")\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the TopicSelector class.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        logger : logging.Logger, optional\n",
    "            Logger object to log activity.\n",
    "        path_logs : pathlib.Path, optional\n",
    "            Path for saving logs.\n",
    "        \"\"\"\n",
    "        self._wmd_model = wmd_model\n",
    "\n",
    "        return\n",
    "\n",
    "    def _get_wmd(self, from_: Union[str, List[str]], to_: Union[str, List[str]], n_words=10) -> float:\n",
    "        \"\"\"\n",
    "        Calculate the Word Mover's Distance between two sentences.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        from_ : Union[str, List[str]]\n",
    "            The source sentence.\n",
    "        to_ : Union[str, List[str]]\n",
    "            The target sentence.\n",
    "        n_words : int\n",
    "            The number of words to consider in the sentences to calculate the WMD.\n",
    "        \"\"\"\n",
    "        if isinstance(from_, str):\n",
    "            from_ = from_.split()\n",
    "\n",
    "        if isinstance(to_, str):\n",
    "            to_ = to_.split()\n",
    "\n",
    "        if n_words < len(from_):\n",
    "            from_ = from_[:n_words]\n",
    "        if n_words < len(to_):\n",
    "            to_ = to_[:n_words]\n",
    "\n",
    "        return self._wmd_model.wmdistance(from_, to_)\n",
    "\n",
    "    def _get_wmd_mat(self, models: list) -> np.ndarray:\n",
    "        \"\"\"Calculate inter-topic distance based topic words using Word Mover's Distance.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        models : list\n",
    "            A list containing two sublists the models. Each sublits is a list of topics, each topic represented as a list of words.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            A matrix of Word Mover's Distance between topics from two models.\n",
    "        \"\"\"\n",
    "\n",
    "        if len(models) != 2:\n",
    "            raise ValueError(\n",
    "                \"models must contain exactly two sublists/arrays.\")\n",
    "\n",
    "        num_topics_first_model = len(models[0])\n",
    "        num_topics_second_model = len(models[1])\n",
    "        wmd_sims = np.zeros((num_topics_first_model, num_topics_second_model))\n",
    "\n",
    "        for k_idx, k in enumerate(models[0]):\n",
    "            for k__idx, k_ in enumerate(models[1]):\n",
    "                wmd_sims[k_idx, k__idx] = self._get_wmd(k, k_)\n",
    "\n",
    "        return wmd_sims\n",
    "\n",
    "    def iterative_matching(self, models, N, remove_topic_ids=None, seed=2357_11):\n",
    "        \"\"\"\n",
    "        Performs an iterative pairing process between the topics of multiple models.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        models : list\n",
    "            A list containing two sublists the models. Each sublits is a list of topics, each topic represented as a list of words.\n",
    "        N : int\n",
    "            Number of matches to find.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list of list of tuple\n",
    "            List of lists with the N matches found. Each match is a list of tuples, where each tuple contains the model index and the topic index.\n",
    "        \"\"\"\n",
    "        random.seed(seed)\n",
    "       \n",
    "        if remove_topic_ids is not None:\n",
    "            modified_models = []\n",
    "            id_mappings = []  # To store mappings for each model\n",
    "            for i_model, model in enumerate(models):\n",
    "                # Create a mapping for this model\n",
    "                mapping = {}\n",
    "                new_model = []\n",
    "                new_topic_id = 0\n",
    "                for i, topic in enumerate(model):\n",
    "                    if i not in remove_topic_ids[i_model]:\n",
    "                        new_model.append(topic)\n",
    "                        # Map new topic ID to the original\n",
    "                        mapping[new_topic_id] = i\n",
    "                        new_topic_id += 1\n",
    "                modified_models.append(new_model)\n",
    "                id_mappings.append(mapping)  # Store the mapping for this model\n",
    "            models = modified_models\n",
    "        else:\n",
    "            id_mappings = [\n",
    "                {i: i for i in range(len(model))}\n",
    "                for model in models\n",
    "            ]\n",
    "        # Case 1: Single Model - Return single-topic matches\n",
    "        if len(models) == 1:\n",
    "\n",
    "            model = models[0]\n",
    "            num_topics = len(model)\n",
    "\n",
    "            if N > num_topics:\n",
    "                raise ValueError(\"N must be less than or equal to the number of topics in the model.\")\n",
    "\n",
    "            selected_topics = random.sample(range(num_topics), N)  # Randomly select N topics\n",
    "\n",
    "            matches = [[(0, topic_id)] for topic_id in selected_topics]  # Wrap each topic in a list\n",
    "\n",
    "            sampled_matches_original = [[(0, id_mappings[0][topic_id])] for topic_id in selected_topics]\n",
    "\n",
    "        # Case 2: Multiple Models - Perform cross-model matching\n",
    "        else:\n",
    "           \n",
    "            self._wmd_model = api.load(self._wmd_model)\n",
    "           \n",
    "            dists = {}\n",
    "            for modelA, modelB in product(range(len(models)), range(len(models))):\n",
    "                dists[(modelA, modelB)] = self._get_wmd_mat(\n",
    "                    [models[modelA], models[modelB]])\n",
    "\n",
    "            matches = []  # Matches with filtered topic IDs\n",
    "\n",
    "            assert (all(N <= len(m) for m in models))\n",
    "            while len(matches) < min(len(m) for m in models):\n",
    "                for seed_model in range(len(models)):\n",
    "                    # Calculate the mean distance to all other models\n",
    "                    min_dists, min_dists_indices = [], []\n",
    "                    for other_model in range(len(models)):\n",
    "                        if seed_model == other_model:\n",
    "                            min_dists_indices.append((seed_model, None))\n",
    "                            continue\n",
    "                        distsAB = dists[(seed_model, other_model)]\n",
    "                        # Get the minimum distance for each topic in the seed model to the other model\n",
    "                        min_dists.append(distsAB.min(1))\n",
    "                        min_dists_indices.append((other_model, distsAB.argmin(1)))\n",
    "                    mean_min_dists = np.mean(min_dists, axis=0)\n",
    "                    seed_model_topic = np.argmin(mean_min_dists)\n",
    "                    seed_model_matches = [\n",
    "                        (model_idx, indices[seed_model_topic]) if model_idx != seed_model else (\n",
    "                            model_idx, seed_model_topic)\n",
    "                        for model_idx, indices in min_dists_indices\n",
    "                    ]\n",
    "                    matches.append(seed_model_matches)\n",
    "\n",
    "                    # Remove the matched topics from the distance matrix\n",
    "                    for modelA, modelA_topic in seed_model_matches:\n",
    "                        for modelB in range(len(models)):\n",
    "                            if modelA != modelB:\n",
    "                                dists[(modelA, modelB)][modelA_topic, :] = np.inf\n",
    "                                dists[(modelB, modelA)][:, modelA_topic] = np.inf\n",
    "\n",
    "            sampled_matches = random.sample(matches, N)\n",
    "\n",
    "            # Map the sampled matches to their original topic IDs (sampled_matches are just positions in the betas matrix)\n",
    "            sampled_matches_original = [\n",
    "                [\n",
    "                    (model_idx, id_mappings[model_idx][topic_id]\n",
    "                    if topic_id is not None else None)\n",
    "                    for model_idx, topic_id in match\n",
    "                ]\n",
    "                for match in sampled_matches\n",
    "            ]\n",
    "\n",
    "            # Output the sampled matches in both forms\n",
    "            print(\"Sampled Matches (Position IDs):\", sampled_matches)\n",
    "            print(\"Sampled Matches (Original Topic IDs):\", sampled_matches_original)\n",
    "\n",
    "        return sampled_matches_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00e0b8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_topic_file(filepath, topn=5):\n",
    "    \"\"\"\n",
    "    Parses a topic file with bilingual topic format.\n",
    "\n",
    "    Returns:\n",
    "        models: list of two models, each is a list of topics (each topic is a list of keywords)\n",
    "    \"\"\"\n",
    "    models = {0: [], 1: []}\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    topic_buffer = {0: None, 1: None}\n",
    "    current_topic = -1\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        # New topic line (e.g., \"1    0.02\")\n",
    "        if line[0].isdigit() and '\\t' not in line and ' ' not in line.split()[0]:\n",
    "            current_topic += 1\n",
    "            topic_buffer = {0: None, 1: None}\n",
    "            continue\n",
    "\n",
    "        parts = line.split()\n",
    "        if len(parts) < 4:\n",
    "            continue\n",
    "\n",
    "        model_idx = int(parts[0])\n",
    "        keywords = parts[3:]\n",
    "\n",
    "        if topn:\n",
    "            keywords = keywords[:topn]\n",
    "\n",
    "        topic_buffer[model_idx] = keywords\n",
    "\n",
    "        # Once both 0 and 1 are populated, we push them to their respective models\n",
    "        if all(topic_buffer.values()):\n",
    "            for idx in (0, 1):\n",
    "                models[idx].append(topic_buffer[idx])\n",
    "            topic_buffer = {0: None, 1: None}\n",
    "\n",
    "    return [models[0], models[1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76e52f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_ZS_file(filepath, topn=None):\n",
    "    \"\"\"\n",
    "    Parses a simple topic file where each line contains:\n",
    "    <topic_id> <keywords...>\n",
    "\n",
    "    Returns:\n",
    "        models: list with a single model (list of topic keyword lists)\n",
    "    \"\"\"\n",
    "    model = []\n",
    "\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if not parts:\n",
    "                continue\n",
    "            keywords = parts[1:]  # skip topic index\n",
    "            if topn:\n",
    "                keywords = keywords[:topn]\n",
    "            model.append(keywords)\n",
    "\n",
    "    return [model]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cecf729",
   "metadata": {},
   "outputs": [],
   "source": [
    "zs_top_path = '/export/usuarios_ml4ds/ammesa/ZS_results/en_2025-06-04_segmented_dataset.parquet.gzip/n_topics_9/ZS_output/topics.txt'\n",
    "m_trans_path = '/export/usuarios_ml4ds/ammesa/mallet_folder/en_2025-06-04_segm_trans/n_topics_50/mallet_output/topickeys.txt'\n",
    "m_match_path = '/export/usuarios_ml4ds/ammesa/mallet_folder/en_2025_06_05_matched/n_topics_50/mallet_output/topickeys.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7211260",
   "metadata": {},
   "outputs": [],
   "source": [
    "tpcs_trans = parse_topic_file(m_trans_path, 10)\n",
    "tpcs_match = parse_topic_file(m_match_path, 10)\n",
    "tpcs_ZS = parse_ZS_file(zs_top_path, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b60a71f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['albany',\n",
       "   'gray',\n",
       "   'baltimore',\n",
       "   'church',\n",
       "   'anglicanism',\n",
       "   'albany_new',\n",
       "   'school',\n",
       "   'alexandria',\n",
       "   'anglican',\n",
       "   'bell'],\n",
       "  ['ley',\n",
       "   'congreso',\n",
       "   'presidente',\n",
       "   'voto',\n",
       "   'constitución',\n",
       "   'estados_unidos',\n",
       "   'coolidge',\n",
       "   'bill_clinton',\n",
       "   'artículo_constitución',\n",
       "   'vicepresidente'],\n",
       "  ['states_constitution',\n",
       "   'article_united',\n",
       "   'amendment',\n",
       "   'constitution',\n",
       "   'supreme_court',\n",
       "   'president',\n",
       "   'clause',\n",
       "   'court',\n",
       "   'congress',\n",
       "   'senate'],\n",
       "  ['american_foxhound',\n",
       "   'hound',\n",
       "   'breed',\n",
       "   'augustus_saint',\n",
       "   'gaudens',\n",
       "   'american_women',\n",
       "   'historical_society',\n",
       "   'archive',\n",
       "   'anti_clericalism',\n",
       "   'state_quarter'],\n",
       "  ['británicos',\n",
       "   'batalla',\n",
       "   'fuerte',\n",
       "   'cochrane',\n",
       "   'mando',\n",
       "   'ejército',\n",
       "   'hombre',\n",
       "   'ataque',\n",
       "   'británico',\n",
       "   'tropas'],\n",
       "  ['espiridón',\n",
       "   'presidente_república',\n",
       "   'arbitraje_derecho',\n",
       "   'espiridón_tremitunte',\n",
       "   'theodoros_deligiannis',\n",
       "   'político_griego',\n",
       "   'seigneur',\n",
       "   'laicos',\n",
       "   'theodoros_pangalos',\n",
       "   'idioma_ruso'],\n",
       "  ['battle',\n",
       "   'army',\n",
       "   'command',\n",
       "   'troop',\n",
       "   'force',\n",
       "   'attack',\n",
       "   'washington',\n",
       "   'fort',\n",
       "   'general',\n",
       "   'retreat'],\n",
       "  ['atlantic_slave',\n",
       "   'trade',\n",
       "   'american_revolution',\n",
       "   'slave',\n",
       "   'slavery',\n",
       "   'britain',\n",
       "   'colony',\n",
       "   'slave_trade',\n",
       "   'lincoln',\n",
       "   'war'],\n",
       "  ['san_petersburgo',\n",
       "   'rusia',\n",
       "   'unión_soviética',\n",
       "   'soviética',\n",
       "   'república_socialista',\n",
       "   'boston',\n",
       "   'federativa_soviética',\n",
       "   'sede',\n",
       "   'ciudad',\n",
       "   'centro']]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpcs_ZS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b78182e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [tpcs_trans[0], tpcs_match[0], tpcs_ZS[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f09d6cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled Matches (Position IDs): [[(0, 10), (1, 9), (2, 6)], [(0, 31), (1, 48), (2, 0)], [(0, 22), (1, 38), (2, 8)], [(0, 6), (1, 25), (2, 7)], [(0, 32), (1, 10), (2, 1)], [(0, 14), (1, 13), (2, 4)]]\n",
      "Sampled Matches (Original Topic IDs): [[(0, 10), (1, 9), (2, 6)], [(0, 31), (1, 48), (2, 0)], [(0, 22), (1, 38), (2, 8)], [(0, 6), (1, 25), (2, 7)], [(0, 32), (1, 10), (2, 1)], [(0, 14), (1, 13), (2, 4)]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[(0, 10), (1, 9), (2, 6)],\n",
       " [(0, 31), (1, 48), (2, 0)],\n",
       " [(0, 22), (1, 38), (2, 8)],\n",
       " [(0, 6), (1, 25), (2, 7)],\n",
       " [(0, 32), (1, 10), (2, 1)],\n",
       " [(0, 14), (1, 13), (2, 4)]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp = TopicSelector()\n",
    "\n",
    "tp.iterative_matching(\n",
    "    models=models,\n",
    "    N = 6\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6993b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3c337e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_idx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
